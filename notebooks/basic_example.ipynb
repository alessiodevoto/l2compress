{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from math import ceil\n",
    "import torch\n",
    "\n",
    "\n",
    "def l2_compress(past_key_values,\n",
    "                   keep_ratio: float = 1,\n",
    "                   prune_after: int = 2048,\n",
    "                   skip_layers: List = [],\n",
    "                   **kwargs):\n",
    "    \"\"\"\n",
    "    Adjust the key value cache for the model.\n",
    "    The function should take in the past key values and return the adjusted key values.\n",
    "    Args:\n",
    "        past_key_values: the past key values from the model. This is a list of tuples, where each tuple contains the key and value tensors.  \n",
    "        keep_ratio: the ratio of tokens to keep for each sequence. Default is 1, which means keep all tokens. ( e.g. If keep_ratio is 0.5, then we keep half of the tokens in each sequence)\n",
    "        prune_after: the number of tokens after which to prune. If seq_len is less than this value, the kv_cache will not be changed by this functioin. Default is 2048.\n",
    "        skip_layers: the layers to skip, i.e. for which we do not prune the kvcache. Default is an empty list.\n",
    "\n",
    "    Returns:\n",
    "        past_key_values: the adjusted past key values.\n",
    "    \"\"\"\n",
    "\n",
    "    # both key and value have shape (batch_size, num_heads, seq_len, head_dim)\n",
    "    # need a list not a tuple\n",
    "    past_key_values = list(past_key_values)\n",
    "   \n",
    "    # iterate over the past key values, should we filter out some layers here ?\n",
    "    for layer, kv in enumerate(past_key_values):\n",
    "\n",
    "        if kv[0].size(2) < prune_after:\n",
    "            continue\n",
    "\n",
    "        keys, values = kv\n",
    "        token_dim = keys.shape[-1]\n",
    "\n",
    "        tokens_to_keep = ceil(keep_ratio * keys.size(2))\n",
    "\n",
    "        # sort kv cache by key norm\n",
    "        token_norms = torch.norm(keys, p=2, dim=-1)\n",
    "\n",
    "        # sort by norm\n",
    "        sorted_indices = token_norms.squeeze(-1).argsort(dim=-1)\n",
    "        sorted_indices_expanded = sorted_indices.unsqueeze(-1).expand(-1, -1, -1, token_dim)\n",
    "\n",
    "        # apply sort\n",
    "        sorted_keys = torch.gather(keys, dim=2, index=sorted_indices_expanded)\n",
    "        sorted_values = torch.gather(values, dim=2, index=sorted_indices_expanded)\n",
    "\n",
    "        if layer not in skip_layers:\n",
    "            past_key_values[layer] = (sorted_keys[:, :, :tokens_to_keep, :], sorted_values[:, :, :tokens_to_keep, :])\n",
    "\n",
    "    return past_key_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens in KV Cache at layer 10 before compression: 13\n",
      "Tokens in KV Cache at layer 10 after compression: 8\n",
      "Tokens in KV Cache at layer 10 after forward pass: 12\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load a pre-trained language model\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3-8B\", torch_dtype=torch.float16).cuda()\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B\")\n",
    "\n",
    "# Forward pass with cache enabled\n",
    "input_ids = tokenizer.encode(\"An embarassingly simple method to compress the KV Cache.\", return_tensors=\"pt\")\n",
    "outputs = model(input_ids.cuda(), use_cache=True)\n",
    "\n",
    "\n",
    "print(\"Tokens in KV Cache at layer 10 before compression:\", outputs.past_key_values[10][0].shape[2])\n",
    "# Compress the KV cache by keeping only the top 90% most significant values\n",
    "compressed_cache = l2_compress(\n",
    "    outputs.past_key_values,    # original KV cache\n",
    "    keep_ratio=0.6,             # percentage of cache to retain based on significance\n",
    "    prune_after=5,              # prune the KV Cache only if it contains more that this amount of tokens\n",
    "    skip_layers=[0, 1]          # skip compression for layers 0 and 1\n",
    ")\n",
    "print(\"Tokens in KV Cache at layer 10 after compression:\", compressed_cache[10][0].shape[2])\n",
    "\n",
    "# Use the compressed KV cache in a subsequent forward pass\n",
    "input_ids = tokenizer.encode(\"Really simple!\", return_tensors=\"pt\")\n",
    "outputs = model(\n",
    "    input_ids.cuda(),                   \n",
    "    past_key_values=compressed_cache,  \n",
    "    use_cache=True               \n",
    ")\n",
    "print(\"Tokens in KV Cache at layer 10 after forward pass:\", outputs.past_key_values[10][0].shape[2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kvcache",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
